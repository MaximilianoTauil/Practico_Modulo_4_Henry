{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practico Integrador Modulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pesentara un entorno Docker con Hadoop (HDFS) y la implementación de:  \n",
    "\n",
    "Lo que haremos sera levantar un entorno de Docker con Hadoop usando diferentes tecnologias que aprendimos en el modulo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer es copiar el repositorio que contiene las herramientas tecnologicas para trabajar, el repositorio que utilizaremos es: https://github.com/lopezdar222/herramientas_big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nos otorga un archivo del tipo docker-compose-V1.yml con todas las configuraciones y conexiones entre si que son necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos ese archivo con el comando | sudo docker-compose -f docker-compose-v1.yml up -d |:\n",
    "* -f es para indicar que no usaremos el archivo por defecto si no el que nosotros poseemos con la configuracion especifica\n",
    "* up es para levantar ese archivo\n",
    "* -d para que se siga ejecutando en segundo plano y no nos tome la terminal es decir que la deje bloqueada.\n",
    "\n",
    "![Composeup1](Imagenes/Ls.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el github que hemos clonado tenemos una carpeta dataset cargado con archivos CSV, debemos copiarla dentro del contenedor namenode pero para copiarla primero vamos a crear una carpeta dentro del contenedor, para ello vamos a ejecutar el comando: |sudo docker cp Datasets namenode:/home/Datasets | donde: \n",
    "* cp significa copy (copiar) y le indicamos el nombre del archivo (Datasets)\n",
    "* Luego indicamos el nombre del contenedor (namenode) y luego de los dos puntos le indicamos el directorio a donde tiene que copiarse\n",
    "\n",
    "![cp_datasets](Imagenes/cp_datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego crearemos un directorio en la raiz de hdfs (sistema de alamcenamiento de archivos de forma distribuida en hadoop), y copiamos los archivos CSV en esa carpeta\n",
    "\n",
    "![hdfs](Imagenes/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segunda parte trabajaremos con Hive, una plataforma de almacenamiento y procesamiento, la cual es parte del ecosistema Hadoop, para ello le daremos de alta al 2do archivo docker-compose para trabajar en ese entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear las tablas dentro de hive debemos posicionarnos en el contenedor hive-server, pero primero debemos copiar de nuestro directorio en el servidor, el script con la creacion de todas las tablas, a continuacion esta la secuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hive](Imagenes/hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el script Paso02.hql con la creacion de todas las tablas, el comando es |hive -f Paso02.hql| hive iniciara el programa, -f indica que se va a proporcionar un archivo con comandos\n",
    "\n",
    "![tablas](Imagenes/tablas_hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parte consiste en que las tablas que creamos anteriormente las podremos almacenar en formato Parquet(binario) ya que es una oportunidad de ocupar menos y tener mayor velocidad de lectura, luego se usara Snappy un programa que comprime y descomprime archivos mas rapido que sus programas alternativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nos entrego un archivo con un script para que todo se ejecute de manera automatica, la modadalidad es la misma que en el paso anterior, copiaremos el archivo dentro del contenedor hive-server y lo ejecutaremos, luego mostraremos como es el script Paso03.hql\n",
    "para ver que se estaria ejecutando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![paso03](Imagenes/paso03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la creacion de una tabla tipo external, con sus columnas y tipos de dato, luego indica:  \n",
    "* \"STORED AS PARQUET\" significa que los datos en la tabla se almacenaran en formato Parquet\n",
    "* \"LOCATION\" nos da la ubicacion donde seran guardados los datos en HDFS, cabe recordar que las tablas external al ser eliminadas se elimina la tabla con su metadata pero no los datos\n",
    "* \"TBLPROPERTIES\" es para asignar propiedades especificas a la tabla en este caso el tipo de compresion que se le aplicara a los datos para guardarlos y es Snappy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asd](Imagenes/asd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte crearemos un indice para la tabla venta especificamente en la columna IdSucursal, el objetivos de los indices es de hacer un acceso mas veloz de la informacion y que el motor de base de datos no tenga que hacer escaneos completos de las tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observa que se selecciona la base de datos a utilizar, luego da un comando: \n",
    "* \"CREATE INDEX\" es la sentencia para crear un indice, luego indica el nombre del indice (index_venta_sucursal)\n",
    "* \"ON TABLE\" se utiliza para señalar que tabla vamos a usar y entre parentesis se indica la columna\n",
    "* \"AS\" lo que hace es especificar el manejador de indices\n",
    "* \"WITH DEFERRED REBUILD\" lo que hace esta sentencia es decirle que no construya en ese momento el indice si no cuando se le indique, esto se hace para cuando se manejan grandes cantidades de datos y se debe de actualizar el indice para que haga las consultas con los nuevos datos ingresados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![indice](Imagenes/indice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levantaremos un nuevo docker compose especifico para esta parte del practico, utilizaremos HBase, MongoDB, Neo4J y Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de dar de alta este nuevo compose, vamos ingresar al contenedor de Hbase y nos metemos en su interfaz de lineas de comando\n",
    "* Crearemos una tabla (personal) y hbase nos pide que si o si al crear una tabla le demos una columna familiar (personal_Data)\n",
    "* Le insertamos informacion de la siguiente forma:\n",
    "  * put <nombre_de_tabla>\n",
    "  * Indicamos \"id\"\n",
    "  * Indicamos a personal_data sus componentes y los datos a insertar\n",
    "* get lo usamos para traer la informacion, le indicamos la tabla y el \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hbase1](Imagenes/hbase1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego colocamos el archivo Personal.csv en hbase/data/personal.csv:\n",
    "* -put es para indicar que el archivo (personal.csv) que esta en nuestro HDFS local se ubique en el HDFS de HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![copy_hbase](Imagenes/copy_hbase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto importamos el archivo csv y especificamos columnas\n",
    "\n",
    "![importamos](Imagenes/importamos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui hacemos un escaneo de la tabla personal para corroborar que realizamos bien la importacion y creamos una nueva tabla con sus columnas familiares, le insertamos datos y realizamos de nuevo un get para ver los datos\n",
    "\n",
    "![scaneo](Imagenes/scaneo.png)\n",
    "\n",
    "![insertaalbumn](Imagenes/insertalbum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos por copiar 2 archivos al contenedor de MongoDb\n",
    "\n",
    "![copy_mongo](Imagenes/copy_mongo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos ubicamos en el contenedor de Mongo e importamos los datos desde un archivo CSV y un archivo JSON en las colecciones \"iris_csv\" e \"iris_json\" en la base de datos \"dataprueba\" de MongoDB\n",
    "\n",
    "![mongo2](Imagenes/mongo2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingresamos al shell de Mongo, cambiamos de base de datos (dataprueba), verificamos que los archivos si se copiaron (Show collections) y luego los accedemos para ver la informacion dentro de ellos\n",
    "\n",
    "![mongosh](Imagenes/mongosh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los archivos de la base de datos Mongo que nos dieron\n",
    "\n",
    "![bdmon](Imagenes/bdmon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una observacion, falta crear un directorio en el contenedor namenode/tmp/udfs, ya que al ejectuar el hive -f iris.hql al ser ejecutado nos dira que no existe tal archivo y deberemos crear la carpeta udfs, ademas de copiar los archivos de mongo a ese directorio tambien\n",
    "\n",
    "![udfs](Imagenes/udfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ultimo punto de Mongo (8) inica el servidor de hive (hiveserver2) y le da permisos totales al archivo iris.hql, pero luego al momento de ejecutar da error, por ende hasta aqui fue realizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajaremos con el Spark y Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos una sesion interactiva de PySpark\n",
    "\n",
    "![spark1](Imagenes/spark1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No me anduvo mas nada que eso🤯, lamento muchisimo no poder hacerlo, creo que con un poco de guia lo podria realizar completo, pero hay errores que nos los entiendo, falta de archivos, los coloco e igual tiran errores, hasta aqui he podido realizar, muchisimas gracias por la oportunidad, prometo dar mas esfuerzo para el proximo"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
